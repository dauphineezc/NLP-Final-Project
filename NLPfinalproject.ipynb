{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dauphineezc/NLP-Final-Project/blob/main/NLPfinalproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnOTeuq4qpQs",
        "outputId": "3148b82e-3313-4cec-ad98-16b8823c2830",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer_six-20250416-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer_six-20250416-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20250416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pdfminer.six spacy scikit-learn\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "import nltk\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from pdfminer.high_level import extract_text  # <-- this is key!\n",
        "from google.colab import files\n",
        "\n",
        "# Load SpaCy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except:\n",
        "    print(\"Please run `!python -m spacy download en_core_web_sm` if this fails.\")\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Gy4vIexLU2-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0199305b-2d61-4c47-fdfa-1e8db86eb4f1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "pdf_path = next(iter(uploaded))  # Grab uploaded filename"
      ],
      "metadata": {
        "id": "Ng_hoJp0iix5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glossary = {\n",
        "    \"mitochondria\": \"An organelle that produces energy for the cell.\",\n",
        "    \"dna\": \"The molecule that carries genetic information.\",\n",
        "    \"enzyme\": \"A protein that speeds up chemical reactions in a cell.\",\n",
        "    \"ribosome\": \"A structure that synthesizes proteins.\",\n",
        "    \"photosynthesis\": \"The process by which green plants convert sunlight into energy.\",\n",
        "    \"atp\": \"A molecule that stores and transfers energy in cells.\",\n",
        "    \"nucleus\": \"The control center of the cell that contains DNA.\",\n",
        "    \"chloroplast\": \"An organelle in plant cells where photosynthesis occurs.\",\n",
        "    \"cell membrane\": \"A barrier that surrounds the cell and controls what enters and leaves.\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "3LV7IS6gnDT5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_term(raw: str) -> str:\n",
        "    \"\"\"\n",
        "    Lowercase, strip leading bullets/non‐alnums,\n",
        "    drop leading 'the/a/an ', and trailing punctuation.\n",
        "    \"\"\"\n",
        "    t = raw.lower().strip()\n",
        "    t = re.sub(r'^[^a-z0-9]+', '', t)           # drop bullets etc.\n",
        "    t = re.sub(r'^(the|a|an)\\s+', '', t)        # drop leading articles\n",
        "    t = re.sub(r'[^\\w\\s]+$', '', t)             # drop trailing punctuation\n",
        "    return t\n",
        "\n",
        "def extract_candidate_terms(doc, top_k=20):\n",
        "    \"\"\"\n",
        "    1) Pull every noun_chunk\n",
        "    2) Normalize them\n",
        "    3) Count freq\n",
        "    4) Take the top_k most common, in order, deduped\n",
        "    \"\"\"\n",
        "    raw_chunks = [chunk.text for chunk in doc.noun_chunks]\n",
        "    norm_chunks = [normalize_term(c) for c in raw_chunks]\n",
        "    # drop any empties\n",
        "    norm_chunks = [c for c in norm_chunks if c]\n",
        "\n",
        "    freq = Counter(norm_chunks)\n",
        "    candidates = []\n",
        "    for term, _ in freq.most_common():\n",
        "        if term not in candidates:\n",
        "            candidates.append(term)\n",
        "        if len(candidates) >= top_k:\n",
        "            break\n",
        "    return candidates\n",
        "\n",
        "def extract_definitions(sentences, candidate_terms):\n",
        "    \"\"\"\n",
        "    Same patterns, but match against lower‐cased sentences\n",
        "    and normalized term keys.\n",
        "    \"\"\"\n",
        "    patterns = [\n",
        "        r\"{term}\\s+is\\s+([^.;]+)\",\n",
        "        r\"{term}\\s+are\\s+([^.;]+)\",\n",
        "        r\"{term}\\s+refers\\s+to\\s+([^.;]+)\",\n",
        "        r\"the\\s+term\\s+{term}\\s+means\\s+([^.;]+)\",\n",
        "    ]\n",
        "    defs = {}\n",
        "    # pre-lower all sentences once\n",
        "    lower_sents = [s.lower() for s in sentences]\n",
        "\n",
        "    for term in candidate_terms:\n",
        "        esc = re.escape(term)\n",
        "        for sent in lower_sents:\n",
        "            for pat in patterns:\n",
        "                m = re.search(pat.format(term=esc), sent)\n",
        "                if m:\n",
        "                    defs[term] = m.group(1).strip()\n",
        "                    break\n",
        "            if term in defs:\n",
        "                break\n",
        "    return defs\n",
        "\n",
        "def generate_flashcards(pdf_path, top_k=20):\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    doc, sentences = preprocess_text(text)\n",
        "    terms = extract_candidate_terms(doc, top_k=top_k)\n",
        "    defs  = extract_definitions(sentences, terms)\n",
        "    return [\n",
        "        {\"term\": term, \"definition\": defs.get(term, \"\")}\n",
        "        for term in terms\n",
        "    ]\n",
        "\n",
        "\n",
        "def check_similarity(user_def, ref_def):\n",
        "    vectorizer = TfidfVectorizer().fit_transform([user_def, ref_def])\n",
        "    return cosine_similarity(vectorizer[0:1], vectorizer[1:2])[0][0]\n",
        "\n"
      ],
      "metadata": {
        "id": "d3RxpymvXD9a"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cards = generate_flashcards(pdf_path)\n",
        "\n",
        "\n",
        "print(\"📘 Flashcard Results:\\n\")\n",
        "\n",
        "for card in cards:\n",
        "    term = card[\"term\"].lower().strip()\n",
        "    user_def = card[\"definition\"]\n",
        "\n",
        "    if not user_def:\n",
        "        continue  # skip empty definitions\n",
        "\n",
        "    print(f\"🧠 Term: {term.title()}\")\n",
        "    print(f\"📝 Definition: {user_def}\")\n",
        "\n",
        "    if term in glossary:\n",
        "        ref_def = glossary[term]\n",
        "        score = check_similarity(user_def, ref_def)\n",
        "        if score < 0.8:\n",
        "            print(f\"⚠️ Warning: Similarity = {score:.2f}. Please double-check this definition.\")\n",
        "        else:\n",
        "            print(f\"✅ Similarity = {score:.2f} — Looks good!\")\n",
        "    else:\n",
        "        print(\"🔍 No reference definition available for this term.\")\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoDwXXO7jDoT",
        "outputId": "bf4fc24d-571d-4d41-f9cb-7bc17f3c84d5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📘 Flashcard Results:\n",
            "\n",
            "🧠 Term: Dna\n",
            "📝 Definition: the molecule that carries genetic information and provides the blueprint for \n",
            "\n",
            "protein synthesis\n",
            "⚠️ Warning: Similarity = 0.63. Please double-check this definition.\n",
            "------------------------------------------------------------\n",
            "🧠 Term: Photosynthesis\n",
            "📝 Definition: the process by which green plants convert sunlight into energy, \n",
            "\n",
            "producing glucose and oxygen\n",
            "⚠️ Warning: Similarity = 0.75. Please double-check this definition.\n",
            "------------------------------------------------------------\n",
            "🧠 Term: Nucleus\n",
            "📝 Definition: the control center of the cell that contains dna\n",
            "✅ Similarity = 1.00 — Looks good!\n",
            "------------------------------------------------------------\n",
            "🧠 Term: Atp\n",
            "📝 Definition: a molecule that stores and transfers energy in cells, fueling processes like \n",
            "\n",
            "muscle contraction and active transport\n",
            "⚠️ Warning: Similarity = 0.64. Please double-check this definition.\n",
            "------------------------------------------------------------\n",
            "🧠 Term: Synthesis\n",
            "📝 Definition: the process by which green plants convert sunlight into energy, \n",
            "\n",
            "producing glucose and oxygen\n",
            "🔍 No reference definition available for this term.\n",
            "------------------------------------------------------------\n",
            "🧠 Term: Chloroplast\n",
            "📝 Definition: an organelle in plant cells where photosynthesis occurs\n",
            "✅ Similarity = 1.00 — Looks good!\n",
            "------------------------------------------------------------\n",
            "🧠 Term: Cell Membrane\n",
            "📝 Definition: a barrier that surrounds the cell and controls what enters and \n",
            "\n",
            "leaves\n",
            "✅ Similarity = 1.00 — Looks good!\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CDfQqnVOFr1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dauphineezc/NLP-Final-Project.git\n",
        "%cd NLP-Final-Project\n",
        "\n",
        "pdf_path = 'NLP sample bio notes.pdf'\n",
        "cards    = generate_flashcards(pdf_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaBaIDfRGbkx",
        "outputId": "d9576872-52f8-4b81-958b-7db6d56b9be8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP-Final-Project'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 12 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (12/12), 74.89 KiB | 2.88 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "/content/NLP-Final-Project\n"
          ]
        }
      ]
    }
  ]
}